Artificial intelligence was born in the 1950s, when a handful
of pioneers from the nascent field of computer science
started asking whether computers could be made to 
“think”—a question whose ramifications we’re still exploring today.
While many of the underlying ideas had been brewing in the
years and even decades prior, “artificial intelligence” finally
crystallized as a field of research in 1956, when John
McCarthy, then a young Assistant Professor of Mathematics
at Dartmouth College, organized a summer workshop under
At the end of the summer, the workshop concluded without
having fully solved the riddle it set out to investigate.
Nevertheless, it was attended by many people who would
move on to become pioneers in the field, and it set in motion
an intellectual revolution that is still ongoing to this day.
Concisely, AI can be described as the effort to automate
intellectual tasks normally performed by humans. As such,
AI is a general field that encompasses machine learning and
deep learning, but that also includes many more approaches
that may not involve any learning. Consider that until the
1980s, most AI textbooks didn’t mention “learning” at all!
Early chess programs, for instance, only involved hardcoded
rules crafted by programmers, and didn’t qualify as machine
learning. In fact, for a fairly long time, most experts believed
that human-level artificial intelligence could be achieved by
having programmers handcraft a sufficiently large set of
explicit rules for manipulating knowledge stored in explicit
databases. This approach is known as symbolic AI. It was
the dominant paradigm in AI from the 1950s to the late
1980s, and it reached its peak popularity during the expert
systems boom of the 1980s.
Although symbolic AI proved suitable to solve well-defined,
logical problems, such as playing chess, it turned out to be
intractable to figure out explicit rules for solving more
complex, fuzzy problems, such as image classification,
speech recognition, or natural language translation. A new
approach arose to take symbolic AI’s place: machine learning.
1.1.2 Machine learning
In Victorian England, Lady Ada Lovelace was a friend and
collaborator of Charles Babbage, the inventor of the
Analytical Engine: the first-known general-purpose
mechanical computer. Although visionary and far ahead of its
time, the Analytical Engine wasn’t meant as a general-
purpose computer when it was designed in the 1830s and
1840s, because the concept of general-purpose computation
was yet to be invented. It was merely meant as a way to use
mechanical operations to automate certain computations
from the field of mathematical analysis—hence the name
Analytical Engine. As such, it was the intellectual descendant
of earlier attempts at encoding mathematical operations in
gear form, such as the Pascaline, or Leibniz’s step reckoner,
a refined version of the Pascaline. Designed by Blaise Pascalin 1642 (at age 19!), the Pascaline was the world’s first
mechanical calculator—it could add, subtract, multiply, or
even divide digits.
In 1843, Ada Lovelace remarked on the invention of the
Analytical Engine,Even with 178 years of historical perspective, Lady
Lovelace’s observation remains arresting. Could a general-
purpose computer “originate” anything, or would it always
be bound to dully execute processes we humans fully
understand? Could it ever be capable of any original
thought? Could it learn from experience? Could it show
creativity?
Her remark was later quoted by AI pioneer Alan Turing as
“Lady Lovelace’s objection” in his landmark 1950 paper
“Computing Machinery and Intelligence,” 1 which introduced
the Turing test as well as key concepts that would come to
shape AI. 2 Turing was of the opinion—highly provocative at
the time—that computers could in principle be made to
emulate all aspects of human intelligence.The usual way to make a computer do useful work is to have
a human programmer write down rules—a computer
program—to be followed to turn input data into appropriate
answers, just like Lady Lovelace writing down step-by-step
instructions for the Analytical Engine to perform. Machine
learning turns this around: the machine looks at the input
Deep learning is a specific subfield of machine learning: a
new take on learning representations from data that puts an
emphasis on learning successive layers of increasingly
meaningful representations. The “deep” in “deep learning”
isn’t a reference to any kind of deeper understanding
achieved by the approach; rather, it stands for this idea of
successive layers of representations. How many layers
contribute to a model of the data is called the depth of the
model. Other appropriate names for the field could have
been layered representations learning or hierarchical
representations learning. Modern deep learning often
involves tens or even hundreds of successive layers of
representations, and they’re all learned automatically from
exposure to training data. Meanwhile, other approaches to
machine learning tend to focus on learning only one or two
layers of representations of the data (say, taking a pixelhistogram and then applying a classification rule); hence,
they’re sometimes called shallow learning.
In deep learning, these layered representations are learned
via models called neural networks, structured in literal layers
stacked on top of each other. The term “neural network”
refers to neurobiology, but although some of the central
concepts in deep learning were developed in part by drawing
inspiration from our understanding of the brain (in particular,
the visual cortex), deep learning models are not models of
the brain. There’s no evidence that the brain implements
anything like the learning mechanisms used in modern deep
learning models. You may come across pop-science articles
proclaiming that deep learning works like the brain or was
modeled after the brain, but that isn’t the case. It would be
confusing and counterproductive for newcomers to the field
to think of deep learning as being in any way related to
neurobiology; you don’t need that shroud of “just like our
minds” mystique and mystery, and you may as well forforget
anything you may have read about hypothetical links
between deep learning and biology. For our purposes, deep
learning is a mathematical framework for learning
representations from data.
What do the representations learned by a deep learning
algorithm look like? Let’s examine how a network several
layers deep (see figure 1.5) transforms an image of a digit in
order to recognize what digit it is.
1.1.5 Understanding how deep learning works, in
three figures
At this point, you know that machine learning is about
mapping inputs (such as images) to targets (such as the
label “cat”), which is done by observing many examples of
input and targets. You also know that deep neural networks
do this input-to-target mapping via a deep sequence of
simple data transformations (layers) and that these data
transformations are learned by exposure to examples. Now
let’s look at how this learning happens, concretely.
The specification of what a layer does to its input data is
stored in the layer’s weights, which in essence are a bunch
of numbers. In technical terms, we’d say that the
transformation implemented by a layer is parameterized by
its weights (see figure 1.7). (Weights are also sometimes
called the parameters of a layer.) In this context, learning
means finding a set of values for the weights of all layers in
a network, such that the network will correctly map example
inputs to their associated targets. But here’s the thing: a
deep neural network can contain tens of millions of
parameters. Finding the correct values for all of them mayseem like a daunting task, especially given that modifying
the value of one parameter will affect the behavior of all the
others!
1.2.1 Probabilistic modeling
Probabilistic modeling is the application of the principles of
statistics to data analysis. It is one of the earliest forms of
machine learning, and it’s still widely used to this day. One
of the best-known algorithms in this category is the Naive Bayes algorithm.
Naive Bayes is a type of machine learning classifier based on
applying Bayes’ theorem while assuming that the features in
the input data are all independent (a strong, or “naive”
assumption, which is where the name comes from). This
form of data analysis predates computers and was applied
by hand decades before its first computer implementation
(most likely dating back to the 1950s). Bayes’ theorem and
the foundations of statistics date back to the eighteenth
century, and these are all you need to start using Naive
Bayes classifiers.
A closely related model is logistic regression (logreg for
short), which is sometimes considered to be the “Hello
World” of modern machine learning. Don’t be misled by its
name—logreg is a classification algorithm rather than a
regression algorithm. Much like Naive Bayes, logreg predates
computing by a long time, yet it’s still useful to this day,
thanks to its simple and versatile nature. It’s often the first
thing a data scientist will try on a dataset to get a feel for
the classification task at hand.
1.2.2 Early neural networks
Early iterations of neural networks have been completely
supplanted by the modern variants covered in these pages,
but it’s helpful to be aware of how deep learning originated.
Although the core ideas of neural networks were
investigated in toy forms as early as the 1950s, the
approach took decades to get started. For a long time, the
missing piece was an efficient way to train large neural
networks. This changed in the mid-1980s, when multiple
people independently rediscovered the Backpropagation
algorithm—a way to train chains of parametric operations
using gradient-descent optimization (we’ll precisely define
these concepts later in the book)—and started applying it to
neural networks.
The first successful practical application of neural nets came
in 1989 from Bell Labs, when Yann LeCun combined the
earlier ideas of convolutional neural networks and
backpropagation, and applied them to the problem of
classifying handwritten digits. The resulting network, dubbed
LeNet, was used by the United States Postal Service in the
1990s to automate the reading of ZIP codes on mail
envelopes.
1.2.3 Kernel methods
As neural networks started to gain some respect among
researchers in the 1990s, thanks to this first success, a new
approach to machine learning rose to fame and quickly sent
neural nets back to oblivion: kernel methods. Kernel
methods are a group of classification algorithms, the best
known of which is the Support Vector Machine (SVM). 
Themodern formulation of an SVM was developed by Vladimir
Vapnik and Corinna Cortes in the early 1990s at Bell Labs
and published in 1995, 3 although an older linear formulation
was published by Vapnik and Alexey Chervonenkis as early
as 1963. 4
SVM is a classification algorithm that works by finding
“decision boundaries” separating two classes (see figure
1.10). SVMs proceed to find these boundaries in two steps:
1. The data is mapped to a new high-dimensional
representation where the decision boundary can be
expressed as a hyperplane (if the data was two-
dimensional, as in figure 1.10, a hyperplane would be a
straight line).
2. A good decision boundary (a separation hyperplane) is
computed by trying to maximize the distance between
the hyperplane and the closest data points from each
class, a step called maximizing the margin. This allows
the boundary to generalize well to new samples outside
of the training dataset.
1.2.4 Decision trees, random forests, and
gradient boosting machines
Decision trees are flowchart-like structures that let you
classify input data points or predict output values given
inputs (see figure 1.11). They’re easy to visualize and
interpret. Decision trees learned from data began to receive
significant research interest in the 2000s, and by 2010 they
were often preferred to kernel methods.
In particular, the Random Forest algorithm introduced a
robust, practical take on decision-tree learning that involves
building a large number of specialized decision trees and
then ensembling their outputs. Random forests are
applicable to a wide range of problems—you could say that
they’re almost always the second-best algorithm for any
shallow machine learning task. When the popular machine
learning competition website Kaggle (http://kaggle.com) got
started in 2010, random forests quickly became a favorite on
the platform—until 2014, when gradient boosting machines
took over. A gradient boosting machine, much like a random
forest, is a machine learning technique based on ensembling
weak prediction models, generally decision trees. It uses
gradient boosting, a way to improve any machine learning
model by iteratively training new models that specialize in
addressing the weak points of the previous models. Applied
to decision trees, the use of the gradient boosting technique
results in models that strictly outperform random forests
most of the time, while having similar properties. It may be
one of the best, if not the best, algorithm for dealing with
nonperceptual data today. Alongside deep learning, it’s one
of the most commonly used techniques in Kaggle competitions.
1.2.5 Back to neural networks
Around 2010, although neural networks were almost
completely shunned by the scientific community at large, a
number of people still working on neural networks started to
make important breakthroughs: the groups of Geoffrey
Hinton at the University of Toronto, Yoshua Bengio at theUniversity of Montreal, Yann LeCun at New York University,
and IDSIA in Switzerland.
In 2011, Dan Ciresan from IDSIA began to win academic
image-classification competitions with GPU-trained deep
neural networks—the first practical success of modern deep
learning. But the watershed moment came in 2012, with the
entry of Hinton’s group in the yearly large-scale image-
classification challenge ImageNet (ImageNet Large Scale
Visual Recognition Challenge, or ILSVRC for short). The
ImageNet challenge was notoriously difficult at the time,
consisting of classifying high-resolution color images into
1,000 different categories after training on 1.4 million
images. In 2011, the top-five accuracy of the winning model,
based on classical approaches to computer vision, was only
74.3%. 5 Then, in 2012, a team led by Alex Krizhevsky and
advised by Geoffrey Hinton was able to achieve a top-five
accuracy of 83.6%—a significant breakthrough. The
competition has been dominated by deep convolutional
neural networks every year since. By 2015, the winner
reached an accuracy of 96.4%, and the classification task on
ImageNet was considered to be a completely solved problem.
Since 2012, deep convolutional neural networks (convnets)
have become the go-to algorithm for all computer vision
tasks; more generally, they work on all perceptual tasks. At
any major computer vision conference after 2015, it was
nearly impossible to find presentations that didn’t involve
convnets in some form. At the same time, deep learning has
also found applications in many other types of problems,
such as natural language processing. It has completely
replaced SVMs and decision trees in a wide range of
applications. For instance, for several years, the European
Organization for Nuclear Research, CERN, used decision
tree–based methods for analyzing particle data from the
ATLAS detector at the Large Hadron Collider (LHC), but
CERN eventually switched to Keras-based deep neural
networks due to their higher performance and ease of
training on large datasets.
1.2.6 What makes deep learning different
The primary reason deep learning took off so quickly is that
it offered better performance for many problems. But that’s
not the only reason. Deep learning also makes problem-
solving much easier, because it completely automates what
used to be the most crucial step in a machine learning
workflow: feature engineering.
Previous machine learning techniques—shallow learning—
only involved transforming the input data into one or two
successive representation spaces, usually via simple
transformations such as high-dimensional non-linear
projections (SVMs) or decision trees. But the refined
representations required by complex problems generally
can’t be attained by such techniques. As such, humans had
to go to great lengths to make the initial input data more
amenable to processing by these methods: they had to
manually engineer good layers of representations for their
data. This is called feature engineering. Deep learning, on
the other hand, completely automates this step: with deep
learning, you learn all features in one pass rather than
having to engineer them yourself. This has greatly simplified
machine learning workflows, often replacing sophisticated
multistage pipelines with a single, simple, end-to-end deep
learning model.
1.2.7 The modern machine learning landscape
A great way to get a sense of the current landscape of
machine learning algorithms and tools is to look at machine
learning competitions on Kaggle. Due to its highly
competitive environment (some contests have thousands of
entrants and million-dollar prizes) and to the wide variety of
machine learning problems covered, Kaggle offers a realistic
way to assess what works and what doesn’t. So what kind of
algorithm is reliably winning competitions? What tools do top
entrants use?
In early 2019, Kaggle ran a survey asking teams that ended
in the top five of any competition since 2017 which primary
software tool they had used in the competition (see figure
1.12). It turns out that top teams tend to use either deep
learning methods (most often via the Keras library) or
gradient boosted trees (most often via the LightGBM or
XGBoost libraries).
1.3 Why deep learning? Why now?
The two key ideas of deep learning for computer vision—
convolutional neural networks and backpropagation—were
already well understood by 1990. The Long Short-Term
Memory (LSTM) algorithm, which is fundamental to deep
learning for timeseries, was developed in 1997 and has
barely changed since. So why did deep learning only take off
after 2012? What changed in these two decades?
In general, three technical forces are driving advances in
machine learning: Hardware
Datasets and benchmarksAlgorithmic advances
Because the field is guided by experimental findings rather
than by theory, algorithmic advances only become possible
when appropriate data and hardware are available to try
new ideas (or to scale up old ideas, as is often the case).
Machine learning isn’t mathematics or physics, where major 
advances can be done with a pen and a piece of paper. It’s an engineering science.
The real bottlenecks throughout the 1990s and 2000s were
data and hardware. But here’s what happened during that
time: the internet took off and high-performance graphics
chips were developed for the needs of the gaming market.
1.3.1 Hardware
Between 1990 and 2010, off-the-shelf CPUs became faster
by a factor of approximately 5,000. As a result, nowadays
it’s possible to run small deep learning models on your
laptop, whereas this would have been intractable 25 years
ago.
But typical deep learning models used in computer vision or
speech recognition require orders of magnitude more
computational power than your laptop can deliver.
Throughout the 2000s, companies like NVIDIA and AMD
invested billions of dollars in developing fast, massively
parallel chips (graphical processing units, or GPUs) to power
the graphics of increasingly photorealistic video games—
cheap, single-purpose supercomputers designed to render
complex 3D scenes on your screen in real time. This
investment came to benefit the scientific community when,
in 2007, NVIDIA launched CUDA
(https://developer.nvidia.com/about-cuda), a programming
interface for its line of GPUs. A small number of GPUs
started replacing massive clusters of CPUs in various highly
parallelizable applications, beginning with physics modeling.
Deep neural networks, consisting mostly of many small
matrix multiplications, are also highly parallelizable, and
around 2011 some researchers began to write CUDA
implementations of neural nets—Dan Ciresan 6 and Alex
Krizhevsky 7 were among the first.
1.3.2 Data
AI is sometimes heralded as the new industrial revolution. If
deep learning is the steam engine of this revolution, then
data is its coal: the raw material that powers our intelligent
machines, without which nothing would be possible. When it
comes to data, in addition to the exponential progress in
storage hardware over the past 20 years (following Moore’s
law), the game changer has been the rise of the internet,
making it feasible to collect and distribute very large
datasets for machine learning. Today, large companies work
with image datasets, video datasets, and natural language
datasets that couldn’t have been collected without the
internet. User-generated image tags on Flickr, for instance,
have been a treasure trove of data for computer vision.
1.3.3 Algorithms
In addition to hardware and data, until the late 2000s, we
were missing a reliable way to train very deep neural
networks. As a result, neural networks were still fairly
shallow, using only one or two layers of representations;
thus, they weren’t able to shine against more-refined
shallow methods such as SVMs and random forests. The key
issue was that of gradient propagation through deep stacks
of layers. The feedback signal used to train neural networks
would fade away as the number of layers increased.
This changed around 2009–2010 with the advent of several
simple but important algorithmic improvements that allowed
for better gradient propagation:
Better activation functions for neural layers
Better weight-initialization schemes, starting with layer-
wise pretraining, which was then quickly abandoned
Better optimization schemes, such as RMSProp and
Adam
Only when these improvements began to allow for training
models with 10 or more layers did deep learning start to
shine.
Finally, in 2014, 2015, and 2016, even more advanced ways
to improve gradient propagation were discovered, such as
batch normalization, residual connections, and depthwise
separable convolutions.
1.3.4 A new wave of investmentAs deep learning became the new state of the art for
computer vision in 2012–2013, and eventually for all
perceptual tasks, industry leaders took note. What followed
was a gradual wave of industry investment far beyond
anything previously seen in the history of AI
In 2011, right before deep learning took the spotlight, the
total venture capital investment in AI worldwide was less
than a billion dollars, which went almost entirely to practical
applications of shallow machine learning approaches. In2015, it had risen to over $5 billion, and in 2017, to a
staggering $16 billion. Hundreds of startups launched in
these few years, trying to capitalize on the deep learning
hype. Meanwhile, large tech companies such as Google,
Amazon, and Microsoft have invested in internal research
departments in amounts that would most likely dwarf the
flow of venture-capital money.
Machine learning—in particular, deep learning—has become
central to the product strategy of these tech giants. In late
2015, Google CEO Sundar Pichai stated, “Machine learning is
a core, transformative way by which we’re rethinking how
we’re doing everything. We’re thoughtfully applying it across
1.3.5 The democratization of deep learning
One of the key factors driving this inflow of new faces in
deep learning has been the democratization of the toolsets
used in the field. In the early days, doing deep learning
required significant C++ and CUDA expertise, which few
people possessed.Nowadays, basic Python scripting skills suffice to do
advanced deep learning research. This has been driven most
notably by the development of the now-defunct Theano
library, and then the TensorFlow library—two symbolic
tensor-manipulation frameworks for Python that support
autodifferentiation, greatly simplifying the implementation of
new models—and by the rise of user-friendly libraries such
as Keras, which makes deep learning as easy as
manipulating LEGO bricks. After its release in early 2015,
Keras quickly became the go-to deep learning solution for large numbers of new startups, graduate students, 
and researchers pivoting into the field.
1.3.6 Will it last?
Is there anything special about deep neural networks that
makes them the “right” approach for companies to be
investing in and for researchers to flock to? Or is deep
learning just a fad that may not last? Will we still be using
deep neural networks in 20 years?
Deep learning has several properties that justify its status as
an AI revolution, and it’s here to stay. We may not be using
neural networks two decades from now, but whatever we
use will directly inherit from modern deep learning and its
core concepts. These important properties can be broadly
sorted into three categories:
Simplicity—Deep learning removes the need for feature
engineering, replacing complex, brittle, engineering-
heavy pipelines with simple, end-to-end trainable models
that are typically built using only five or six different
tensor operations.
Scalability—Deep learning is highly amenable to
parallelization on GPUs or TPUs, so it can take full
advantage of Moore’s law. In addition, deep learning
models are trained by iterating over small batches of
data, allowing them to be trained on datasets of
arbitrary size. (The only bottleneck is the amount of
parallel computational power available, which, thanks to
Moore’s law, is a fast-moving barrier.)
Versatility and reusability—Unlike many prior machine
learning approaches, deep learning models can be
trained on additional data without restarting from
scratch, making them viable for continuous online
learning—an important property for very large
production models. Furthermore, trained deep learning
models are repurposable and thus reusable: for
instance, it’s possible to take a deep learning model
trained for image classification and drop it into a video-
processing pipeline. This allows us to reinvest previous
work into increasingly complex and powerful models.
This also makes deep learning applicable to fairly small datasets.
3 Introduction to Keras and TensorFlow
This chapter covers
A closer look at TensorFlow, Keras, and their relationship
Setting up a deep learning workspace
An overview of how core deep learning concepts translate to
Keras and TensorFlow
This chapter is meant to give you everything you need to
start doing deep learning in practice. I’ll give you a quick
presentation of Keras (https://keras.io) and TensorFlow
(https://tensorflow.org), the Python-based deep learning
tools that we’ll use throughout the book. You’ll find out how
to set up a deep learning workspace, with TensorFlow, Keras,
and GPU support. Finally, building on top of the first contact
you had with Keras and TensorFlow in chapter 2, we’ll review
the core components of neural networks and how they
translate to the Keras and TensorFlow APIs.
By the end of this chapter, you’ll be ready to move on to
practical, real-world applications, which will start with
chapter 4.
3.1 What’s TensorFlow?
TensorFlow is a Python-based, free, open source machine
learning platform, developed primarily by Google. Much like
NumPy, the primary purpose of TensorFlow is to enable
engineers and researchers to manipulate mathematical
expressions over numerical tensors. But TensorFlow goes far
beyond the scope of NumPy in the following ways:
It can automatically compute the gradient of any
differentiable expression (as you saw in chapter 2),
making it highly suitable for machine learning.
It can run not only on CPUs, but also on GPUs and TPUs,
highly parallel hardware accelerators.
Computation defined in TensorFlow can be easily
distributed across many machines.
TensorFlow programs can be exported to other runtimes,
such as C++, JavaScript (for browser-based
applications), or TensorFlow Lite (for applications
running on mobile devices or embedded devices), etc.
This makes TensorFlow applications easy to deploy in
practical settings.
It’s important to keep in mind that TensorFlow is much more
than a single library. It’s really a platform, home to a vast
ecosystem of components, some developed by Google and
some developed by third parties. For instance, there’s TF-
Agents for reinforcement-learning research, TFX for
industry-strength machine learning workflow management,
TensorFlow Serving for production deployment, and there’s
the TensorFlow Hub repository of pretrained models.
Together, these components cover a very wide range of usecases, from cutting-edge research to large-scale production
applications.
TensorFlow scales fairly well: for instance, scientists from
Oak Ridge National Lab have used it to train a 1.1 exaFLOPS
extreme weather forecasting model on the 27,000 GPUs of
the IBM Summit supercomputer. Likewise, Google has used
TensorFlow to develop very compute-intensive deep learning
applications, such as the chess-playing and Go-playing agent
AlphaZero. For your own models, if you have the budget,
you can realistically hope to scale to around 10 petaFLOPS
on a small TPU pod or a large cluster of GPUs rented on
Google Cloud or AWS. That would still be around 1% of the
peak compute power of the top supercomputer in 2019!
3.2 What’s Keras?
Keras is a deep learning API for Python, built on top of
TensorFlow, that provides a convenient way to define and
train any kind of deep learning model. Keras was initially
developed for research, with the aim of enabling fast deep learning experimentation.
Through TensorFlow, Keras can run on top of different types
of hardware (see figure 3.1)—GPU, TPU, or plain CPU—and
can be seamlessly scaled to thousands of machines.
Keras is known for prioritizing the developer experience. It’s
an API for human beings, not machines. It follows best
practices for reducing cognitive load: it offers consistent and
simple workflows, it minimizes the number of actions
required for common use cases, and it provides clear and
actionable feedback upon user error. This makes Keras easy
to learn for a beginner, and highly productive to use for an
expert.
Keras has well over a million users as of late 2021, ranging
from academic researchers, engineers, and data scientists at
both startups and large companies to graduate students and
hobbyists. Keras is used at Google, Netflix, Uber, CERN,
NASA, Yelp, Instacart, Square, and hundreds of startups
working on a wide range of problems across every industry.
Your YouTube recommendations originate from Keras
models. The Waymo self-driving cars are developed with
Keras models. Keras is also a popular framework on Kaggle,
the machine learning competition website, where most deep
3.3 Keras and TensorFlow: A brief history
Keras predates TensorFlow by eight months. It was released
in March 2015, and TensorFlow was released in November
2015. You may ask, if Keras is built on top of TensorFlow,
how it could exist before TensorFlow was released? Keras
was originally built on top of Theano, another tensor-
manipulation library that provided automatic differentiation
and GPU support—the earliest of its kind. Theano, developed
at the Montréal Institute for Learning Algorithms (MILA) at
the Université de Montréal, was in many ways a precursor of
TensorFlow. It pioneered the idea of using static computation
graphs for automatic differentiation and for compiling code
to both CPU and GPU.
In late 2015, after the release of TensorFlow, Keras was
refactored to a multibackend architecture: it became
possible to use Keras with either Theano or TensorFlow, and
switching between the two was as easy as changing an
environment variable. By September 2016, TensorFlow had
reached a level of technical maturity where it became
possible to make it the default backend option for Keras. In
2017, two new additional backend options were added to
Keras: CNTK (developed by Microsoft) and MXNet
(developed by Amazon). Nowadays, both Theano and CNTK
are out of development, and MXNet is not widely used
outside of Amazon. Keras is back to being a single-backend
API—on top of TensorFlow.
Keras and TensorFlow have had a symbiotic relationship for
many years. Throughout 2016 and 2017, Keras became well
known as the user-friendly way to develop TensorFlow
applications, funneling new users into the TensorFlow
ecosystem. By late 2017, a majority of TensorFlow users
were using it through Keras or in combination with Keras. In
2018, the TensorFlow leadership picked Keras as
TensorFlow’s official high-level API. As a result, the Keras API
is front and center in TensorFlow 2.0, released in September
2019—an extensive redesign of TensorFlow and Keras that
takes into account over four years of user feedback and technical progress.
3.4 Setting up a deep learning workspace
Before you can get started developing deep learning
applications, you need to set up your development
environment. It’s highly recommended, although not strictly
necessary, that you run deep learning code on a modern
NVIDIA GPU rather than your computer’s CPU. Some
applications—in particular, image processing with
convolutional networks—will be excruciatingly slow on CPU,
even a fast multicore CPU. And even for applications thatcan realistically be run on CPU, you’ll generally see the
speed increase by a factor of 5 or 10 by using a recent GPU.
To do deep learning on a GPU, you have three options:
Buy and install a physical NVIDIA GPU on your
workstation.
Use GPU instances on Google Cloud or AWS EC2.
Use the free GPU runtime from Colaboratory, a hosted
notebook service offered by Google (for details about
what a “notebook” is, see the next section).
Colaboratory is the easiest way to get started, as it requires
no hardware purchase and no software installation—just open a tab in your browser and start coding. It’s the option
we recommend for running the code examples in this book.
However, the free version of Colaboratory is only suitable for
small workloads. If you want to scale up, you’ll have to use
the first or second option.
If you don’t already have a GPU that you can use for deep
learning (a recent, high-end NVIDIA GPU), then running
deep learning experiments in the cloud is a simple, low-cost
way for you to move to larger workloads without having to
buy any additional hardware. If you’re developing using
Jupyter notebooks, the experience of running in the cloud is
no different from running locally.
But if you’re a heavy user of deep learning, this setup isn’t
sustainable in the long term—or even for more than a few
months. Cloud instances aren’t cheap: you’d pay $2.48
3.4.1 Jupyter notebooks: The preferred way to run
deep learning experiments
Jupyter notebooks are a great way to run deep learning
experiments—in particular, the many code examples in this
book. They’re widely used in the data science and machine
learning communities. A notebook is a file generated by the
Jupyter Notebook app (https://jupyter.org) that you can edit
in your browser. It mixes the ability to execute Python code
with rich text-editing capabilities for annotating what you’re
doing. A notebook also allows you to break up longexperiments into smaller pieces that can be executed
independently, which makes development interactive and
means you don’t have to rerun all of your previous code if
something goes wrong late in an experiment.
I recommend using Jupyter notebooks to get started with
Keras, although that isn’t a requirement: you can also run
standalone Python scripts or run code from within an IDE
such as PyCharm. All the code examples in this book are
available as open source notebooks; you can download them
from GitHub at github.com/fchollet/deep-learning-with-
python-notebooks.
3.4.2 Using Colaboratory
Colaboratory (or Colab for short) is a free Jupyter notebook
service that requires no installation and runs entirely in the
cloud. Effectively, it’s a web page that lets you write and
execute Keras scripts right away. It gives you access to a
free (but limited) GPU runtime and even a TPU runtime, so
you don’t have to buy your own GPU. Colaboratory is what
we recommend for running the code examples in this book.
FIRST STEPS WITH COLABORATORY
INSTALLING PACKAGES WITH PIP
The default Colab environment already comes with
TensorFlow and Keras installed, so you can start using it
right away without any installation steps required. But if you
ever need to install something with pip , you can do so by
using the following syntax in a code cell (note that the line
starts with ! to indicate that it is a shell command rather than Python code): !pip install package_name
3.5 First steps with TensorFlow
As you saw in the previous chapters, training a neural
network revolves around the following concepts:
First, low-level tensor manipulation—the infrastructure
that underlies all modern machine learning. This
translates to TensorFlow APIs:
Tensors, including special tensors that store the network’s state (variables)
Tensor operations such as addition, relu , matmul
Backpropagation, a way to compute the gradient of
mathematical expressions (handled in TensorFlow via
the GradientTape object) Second, high-level deep learning concepts. This
translates to Keras APIs: Layers, which are combined into a model
A loss function, which defines the feedback signal used for learning
An optimizer, which determines how learning proceeds
Metrics to evaluate model performance, such as accuracy
A training loop that performs mini-batch stochastic gradient descent
In the previous chapter, you already had a first light contact
with some of the corresponding TensorFlow and Keras APIs:
you’ve briefly used TensorFlow’s Variable class, the
matmul operation, and the GradientTape . You’ve instantiated Keras Dense layers, packed them into a
Sequential model, and trained that model with the fit() method.
Now let’s take a deeper dive into how all of these different
concepts can be approached in practice using TensorFlow and Keras.
3.5.1 Constant tensors and variables
To do anything in TensorFlow, we’re going to need some
tensors. Tensors need to be created with some initial value.
For instance, you could create all-ones or all-zeros tensors
